{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1:\n",
    "Write a function that implements the 1-nearest neighbor classifier with Euclidean distance.\n",
    "Your function should take as input a matrix of training feature vectors X and a vector of the\n",
    "corresponding labels Y, as well as a matrix of test feature vectors test. The output should be a\n",
    "vector of predicted labels preds for all the test points. Naturally, you should not use (or look at the\n",
    "source code for) any library functions for computing Euclidean distances, nearest neighbor queries,\n",
    "and so on. If in doubt about what is okay to use, just ask.\n",
    "\n",
    "Instead of using your 1-NN code directly with data and labels as the training data, do the\n",
    "following. For each value n ∈ {1000, 2000, 4000, 8000},\n",
    " - Draw n random points from data, together with their corresponding labels. In Python, use sel = random.sample(xrange(60000),n)\n",
    "(after import random), ocr['data'][sel], and ocr['labels'][sel].\n",
    " - Use these n points as the training data and testdata as the test points, and compute the\n",
    "test error rate of the 1-NN classifier.\n",
    "\n",
    "A plot of the error rate (on the y-axis) as a function of n (on the x-axis) is called a learning curve.\n",
    "We get an estimate of this curve by using the test error rate in place of the (true) error rate.\n",
    "Since the above process involves some randomness, you should repeat it independently several\n",
    "times (say, at least ten times). Produce an estimate of the learning curve plot using the average of\n",
    "these test error rates (that is, averaging over the repeated trials). Add error bars to your plot that\n",
    "extend to one standard deviation above and below the means. Ensure the plot axes are properly\n",
    "labeled. Submit the plot and your source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "ocr = loadmat('./homework1/data/ocr.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testdata = ocr['testdata']\n",
    "testlabels = ocr['testlabels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_image(X):\n",
    "    plt.imshow(X.reshape((28, 28)), cmap=cm.gray_r )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_distances(x, Y):\n",
    "    \"\"\" Computes the euclidian ditances between a test vector \n",
    "    and list of training vectors \n",
    "    \"\"\"\n",
    "    return np.sqrt(((x-Y)**2).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sel = random.sample(xrange(60000), 1000) \n",
    "trainingdata = ocr['data'][sel]\n",
    "traininglabels = ocr['labels'][sel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = trainingdata\n",
    "Y = testdata[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oldidsts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "oldidsts = cdist(X, Y, 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del euclidean_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newdists = euclidean_dist(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.dot(X, X.T)\n",
    "b = np.dot(X, Y.T)\n",
    "c = np.dot(Y, Y.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ad = np.tile(np.diagonal(a), (Y.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cd = np.tile(np.diagonal(c), (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2000)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ad.T - 2 * b + cd).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_dist(X, Y):\n",
    "    \"\"\" Computes the euclidean distance \n",
    "    Args:\n",
    "        X: matrix of training data\n",
    "        Y: matrix of test data\n",
    "        \n",
    "    Returns:\n",
    "        dists: matrix of euclidean distances\n",
    "    \"\"\"\n",
    "    a = np.dot(X, X.T)\n",
    "    b = np.dot(X, Y.T)\n",
    "    c = np.dot(Y, Y.T)\n",
    "    ad = np.tile(np.diagonal(a), (Y.shape[0] ,1))\n",
    "    cd = np.tile(np.diagonal(c), (1, 1))\n",
    "    return ad.T - 2 * b + cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_training_data(sample_size, collection_size):\n",
    "    collection = []\n",
    "    for sample in range(collection_size):\n",
    "        sel = random.sample(xrange(60000), sample_size) \n",
    "        data = ocr['data'][sel]\n",
    "        labels = ocr['labels'][sel]\n",
    "        collection.append({'data':data, 'labels':labels})\n",
    "    \n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_error_rates(sample_size):\n",
    "    actual = [lab[0] for lab in ocr['testlabels']]\n",
    "    error_rates = []\n",
    "    for training in generate_training_data(sample_size, 10):\n",
    "        trainingdata = training['data']\n",
    "        traininglabels = training['labels']\n",
    "        preds = nearest_neighbors(ocr['testdata'], trainingdata, traininglabels)\n",
    "        error_rate = compute_error_rate(preds, actual)\n",
    "        error_rates.append(error_rate)\n",
    "    \n",
    "    return error_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error_rate(preds, actual):\n",
    "    zipped = zip(preds, actual)\n",
    "    errors = [tup for tup in zipped if tup[0] != tup[1]]\n",
    "    \n",
    "    return len(errors) / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearest_neighbors(testdata, trainingdata, traininglabels):\n",
    "    dists = cdist(testdata, trainingdata, 'euclidean')\n",
    "#     dists = euclidean_dist(trainingdata, testdata)\n",
    "    nnixs = np.argmin(dists, axis=1)\n",
    "    preds = [traininglabels[ix][0] for ix in nnixs]\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "onethousand_errors = compute_error_rates(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twothousand_errors = compute_error_rates(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fourthousand_errors = compute_error_rates(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eightthousand_errors = compute_error_rates(8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "onek = pd.Series(onethousand_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twok = pd.Series(twothousand_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fourk = pd.Series(fourthousand_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eightk = pd.Series(eightthousand_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'1000': onek, '2000': twok, '4000': fourk, '8000': eightk})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sns.tsplot(data=data.mean(), err_style=\"ci_bars\", interpolate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = data.mean().reset_index()\n",
    "df.columns = ['sample_size', 'error']\n",
    "melted = pd.melt(data)\n",
    "melted.columns = ['sample_size', 'error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sns.set(rc={'axes.facecolor':'white', 'figure.facecolor':'white'})\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x112d6b410>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFtCAYAAAAeffM/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XlYVGX/BvB7NtZhdUcEN0BNQcEVQQUlpdWtQAs1K5O0\nzNx3Sw3UytfXpbJMfmG9gObSXiKIKOQCIuICigsqiCiCMGwDM78/yKOTCi4Mwwz357q8Ls/zzJz5\nwog358w530ekVqvVICIiIr0n1nUBREREVDcY6kRERAaCoU5ERGQgGOpEREQGgqFORERkIBjqRERE\nBkKq6wKIqG516tQJTk5OkEgkGuMbN26EnZ1dvbyuSCRCaWkp5HI5li5diq5du9b43G3btkGpVGLs\n2LFaq4+oMWCoExmg8PBwWFtb6/x1v/32WyxfvhwRERE1Pi8pKQnOzs7aLo/I4DHUiQzQw3pKHTp0\nCCtWrICZmRlKS0sxa9YsrFq1CmZmZigrK0NUVBR27tyJrVu3QiwWo2nTpli0aBHatm2LuXPnoqCg\nAFeuXIGPjw9mzJhR4+tWVlYiOztbCPkbN25g8eLFyM/PR15eHuzs7LB27VokJSUhNjYWCQkJMDEx\nwdixY/HFF19gz549UKlUaN26NZYsWYLmzZtr55tFZEAY6kQGaNy4cRqn39u0aYN169YBAM6dO4e9\ne/eiVatWOHTokMZ2YmIiNm/ejMjISNjY2GDnzp2YMmUKfv31VwBARUUFfvnllxpfVywWIz8/H8bG\nxvDx8cEnn3wCAPjtt9/g7u6Ot956CwAwadIk7N69G2+88QZiYmLg7OyMsWPHYteuXTh79iy2bdsG\niUSCyMhILFy4EJs2bdLWt4vIYDDUiQxQTaffW7ZsiVatWj1wOz4+Hs899xxsbGwAACNGjMCKFStw\n5coViEQiuLu7P9Lrnj59Gm+//TZ69OgBW1tbANWBf/ToUWzZsgUXL17E2bNn4ebmdt8+YmNjceLE\nCYwaNQoAUFVVhfLy8sf/JhA1Qgx1okbG3Nz8odtqtfq+U/dqtRqVlZUAADMzs0d6jc6dO2PevHlY\nsGAB3Nzc0Lp1a6xevRonTpzA6NGj0bdvX1RVVT3wYwK1Wo1JkyYhMDAQQPXZgdu3bz/W10jUWPGW\nNiID9KTrNHl7e+P3339Hfn4+AODHH3+EjY0NHB0dH3ufzz//PHr06CGcfj948CDGjx+Pl156Cba2\ntkhISIBKpQIASCQSKJVKAICXlxeioqJQXFwMAFi7di1mz579RF8PUWPDI3UiA/Tvz9QBYPr06TA1\nNa3xeZ6enhg/fjzGjx8PtVoNW1tbfPXVVxCJRMKfh3nQ3KJFi/DSSy/h4MGDmDJlClatWoUNGzZA\nKpXCw8MDly5dAgAMGDAAH3/8MQDg7bffRm5uLgICAiASiWBnZ4fQ0NDH/RYQNUoiLr1KRERkGLR2\n+l2lUmHx4sUIDAxEUFAQsrKy7ntMfn4+hg4dioqKCgBASUkJgoOD8frrr+ONN95Abm6utsojIiIy\nOFoL9ejoaCiVSkRERGDmzJn3nT6Lj4/HxIkTcfPmTWFs27Zt6NatG7Zu3YqXXnoJ33zzjbbKIyIi\nMjha+0w9OTkZ3t7eAAA3NzekpaVpzEskEoSFhWHkyJHC2Pjx44ULZ65evQorKyttlUdERGRwtBbq\nxcXFkMvlwrZEIoFKpYJYXH1ywNPT84HPE4vFGDduHM6dO4dvv/1WW+UREREZHK2Fulwuh0KhELbv\nDfTafPfddzh//jzeeecd7Nmzp8bHJiUlPVWdRERE+sbDw+OB41oLdXd3d8TGxsLf3x8pKSlwcXGp\n9TmbNm1CixYt8PLLL8PMzOy+W3Ie5mFfHBERkaGp6WBWa6Hu5+eHgwcPCl2hQkJCEBYWBgcHB/j6\n+gqPu/fe1lGjRmHOnDnYvn07VCoVQkJCtFUeERGRwdH7+9STkpJ4pE5ERI1GTbnHNrFEREQGgqFO\nRERkIBjqREREBoKhTkREZCAY6kRERAaCoU5ERGQgGOpEREQGgqFORERkIBjqREREBoKhTkREZCAY\n6kRERAaCoU5ERGQgGOpEREQGgqFORERkIBjqREREBoKhTkREZCAY6kRERAaCoU5ERGQgGOpEREQG\ngqFORERkIBjqREREBoKhTkREZCAY6kRERAaCoU5ERGQgGOpEREQGgqFORERkIBjqREREBoKhTkRE\nZCAY6kRERAaCoU5ERGQgtBbqKpUKixcvRmBgIIKCgpCVlXXfY/Lz8zF06FBUVFQAAIqKijB58mQE\nBQUhMDAQKSkp2iqPiIjI4Ggt1KOjo6FUKhEREYGZM2ciNDRUYz4+Ph4TJ07EzZs3hbGwsDB4enoi\nPDwcISEh+Pjjj7VVHhERkcGRamvHycnJ8Pb2BgC4ubkhLS1NY14ikSAsLAwjR44UxiZMmAAjIyMA\nQGVlJYyNjbVVHhERkcHRWqgXFxdDLpcL2xKJBCqVCmJx9ckBT0/P+55jYWEBAMjLy8Ps2bOxYMEC\nbZVHRERkcLQW6nK5HAqFQti+N9Brkp6ejhkzZmDOnDno2bPnI71WUlLSE9dJRERkKLQW6u7u7oiN\njYW/vz9SUlLg4uJS63POnTuHadOmYe3atY/0+Ds8PDyeplQiIiK9UdOBrNZC3c/PDwcPHkRgYCAA\nICQkBGFhYXBwcICvr6/wOJFIJPz9888/h1KpxPLlywEAlpaW2LBhg7ZKJCIiMigitVqt1nURTyMp\nKYlH6kRE1GjUlHtsPkNERGQgGOoN2Jc7UvHijN34ckeqrkshIiI9wFBvoErLK/FbwgUAwO8JF1Ba\nXqnjioiIqKFjqDdQykoV7lztoFJXbxMREdWEoU5ERGQgGOpEREQGgqFORERkIBjqREREBoKhTkRE\nZCAY6kRERAaCoU5ERGQgGOoN1MXsQl2XQEREeoah3kB98a/WsHq+7g4REdUDhnoDZSU30tje/FMa\nqlQMdiIiejiGegMVPNJVYzs26Qo++z6J7WKJiOihGOoNlI2l6X1j8SlXsWLLIZRVcHEXIiK6H0Nd\nzySduY7FXyWiuFSp61KIiKiBYajrCXeX5sLfT1/Mx4KNB3GrqEyHFRERUUPDUNcT0wJ7YJCHvbB9\nPrsQc9cfwPVbJTqsioiIGhKGup6QSsSYHuiOF/q3E8aybygwZ108LucW6bAyIiJqKBjqekQsFmHS\niG4I9HMRxm4UlmHuhgM4d6VAh5UREVFDwFDXMyKRCK8N64S3Xu4qjN1WVGD+xoNIy7yhw8qIiEjX\nGOp66uUBHTAtoDvEourt0vJKLNmUiCOnrum2MCIi0hmGegMlk4oh+iewxaLq7X8b0tsRc8b1glRS\nPVdRqcKKLYcRl3ylPkslIqIGgqHeQJkaS/GcZ/VFcf6e7WBqLH3g4zxd7bDkrT4wMZIAAKpUanz2\nQxJ+S7hQb7USEVHDIFLr+UohSUlJ8PDw0HUZOnfmUj4++vpvjaY0Qf6d8cpgJ4juHPITEZHeqyn3\neKRuIDo52iJkihdsLIyFsfDfT2PLL6e4whsRUSPBUDcgbVtZYuVUb7SwNRPGdu47h3VRKVzhjYio\nEWCoG5hWTc2xcqoXHFpaCGN7DmdhdfhRKCurdFgZERFpG0PdADWxMkXIu15wdrAWxg6mZmPZ5kMo\nK+cKb0REhkrroa5SqbB48WIEBgYiKCgIWVlZ9z0mPz8fQ4cORUVFhTB26dIlvPjii9ouz2BZmhth\n2TuecO3YVBg7lpGHxZsSUVxSUcMziYhIX2k91KOjo6FUKhEREYGZM2ciNDRUYz4+Ph4TJ07EzZs3\nhbFdu3bhww8/xK1bt7RdnkEzM5FhyVt90bdrS2Hs9MV8zNt4ELduc4U3IiJDo/VQT05Ohre3NwDA\nzc0NaWlpGvMSiQRhYWGwtLQUxqytrbF161Ztl9YoGMkkmDuuF3x7thHGLubcxpwNB5CbzxXeiIgM\nidZDvbi4GHK5XNiWSCRQqVTCtqenJ6ytrTWeM2jQIJiammq7tEZDIhFjWkAPvOTdXhjLuaHAnPXx\nyLp2W4eVERFRXdJ6qMvlcigUCmFbpVJBLOb1efVNLBbhrZe7YuzQTsLYzcIyzN1wEGcv82MOIiJD\n8ODeo3XI3d0dsbGx8Pf3R0pKClxcXGp/0mNKSkqq830aKucmwDAPK/yRVAgAKCqpwNz18RgzsAna\ntTDRcXVERPQ0tB7qfn5+OHjwIAIDAwEAISEhCAsLg4ODA3x9fYXHPU0rU7aJfTweHkBn5yysjUyB\nSqVGRaUaP8TlY+64Xuj9TMvad0BERDpT04Ese783Yn+n5WDld0dRWVV9jYNYLMIHgT3g49GmlmcS\nEZGusPc7PVDfrq2w9O2+MDWuXuFNpVLj8x+S8cuB8zqujIiIngRDvZFzc2qG5ZP7w8JMJox9tfME\nIvekcyEYIiI9w1AnODvYIGSKF2wt714ot/WPM9j800kGOxGRHmGoEwDAsaUlVk71Qssmd1d4270/\nE/+NTEFVlaqGZxIRUUPBUCdByybmWDnVG21b3e3uF30kCyu5whsRkV5gqJMGW0sTfPJuf7g42ghj\niSdy8PE3h1DKFd6IiBo0hjrdx8KseoW37s7NhLGUs3lY9FUCirjCGxFRg8VQpwcyNZZi8Zt90K9b\nK2Es/dItzNtwAPlc4Y2IqEFiqNNDyaQSzAnqiSG9HISxS9eKMGd9PK7dVNTwTCIi0gWGOtVIIhHj\n/YDuGD6wgzB27WYJ5qyPxyWu8EZE1KAw1KlWIpEIE198Bq/7313hLf92OeZtOICMLK7wRkTUUDDU\n6ZGIRCIEDHHB5BHdhLGiEiUWfnkQx8/m6bAyIiK6g6FOj+V5r/aYMdYdYnH1qnql5VVY+vXfSDyR\no+PKiIiIoU6PbZBHGyyY0BsyafU/n8oqFUK/O4KYo1k6royIqHFjqNMT6f1MS3z0dj+YGksBVK/w\ntuZ/x/BTfKaOKyMiarwY6vTEunVsihXBnrAwMxLGvt6Vhv/9eYYLwRAR6QBDnZ6KUxsbrJzqhSZW\nd1d4++GvdHy9Ow0qFYOdiKg+MdTpqbVpYYGVU73Rqqm5MPZz/HmsjTzGFd6IiOoRQ53qRAtbM6yc\n4qWxwlvM0csI/e4IKpRc4Y2IqD4w1KnO2FiaIGSKFzq3tRXG/k67ho+++RslZUodVkZE1Dgw1KlO\nyU1l+HhSP7i7NBfGUs/dwKKvEnBbwRXeiIi0iaFOdc7EWIqFE/ugv5udMJaRVYB5Gw/gZmGpDisj\nIjJsDHXSCplUjFmv98SzfRyFsaxrRZiz/gBybnCFNyIibWCok9ZIxCJMfcUNIwd1FMZy86tXeLuY\nwxXeiIjqGkOdtEokEmHCC10w7rnOwtitouoV3s5cytdhZUREhoehTlonEonwymBnBI9yhah6HRgU\nlyqx6MsEpGRc121xREQGhKFO9eY5z3aYMdYDkn9WeCurqMJH3xxCQmq2jisjIjIMDHWqVwPd7bHg\njd4wumeFt5XfHUH04Us6royISP8x1Kne9erSEh9N6gczk39WeFMDayNTsCuOK7wRET0NhjrpRNcO\nTbEiuD8sze+u8Lb5pzRs/eM0V3gjInpCDHXSmY721gid4oWm1qbCWOSeDGzaeYIrvBERPQGthbpK\npcLixYsRGBiIoKAgZGVl3feY/Px8DB06FBUV1e1Dy8rK8N577+G1117DpEmTkJ/PW54MXfUKb15o\n3ezuCm+/HLyANRHJqOQKb0REj0VroR4dHQ2lUomIiAjMnDkToaGhGvPx8fGYOHEibt68KYz973//\ng4uLC77//nsMHz4cX3zxhbbKowakuY0ZQqd4o72dlTC2L+kKQsK4whsR0ePQWqgnJyfD29sbAODm\n5oa0tDSNeYlEgrCwMFhaWmo8Z8CAAQAAb29vJCYmaqs8amCsLYyx4t3+6NLu7gpvh09dw9KvucIb\nEdGj0lqoFxcXQy6XC9sSiQQq1d3TqZ6enrC2tn7oc8zNzVFUVKSt8qgBkpvK8NGkfvDodHeFtxOZ\nN7DgywQUFpfrsDIiIv0g1daO5XI5FIq7C3eoVCqIxTX/DiGXy1FcXAwAUCgUGkfxNUlKSnryQqnB\n8XeToazEFCezqld0O3e5AB98Fo0g36awMtPaP1kiIr2ntf8h3d3dERsbC39/f6SkpMDFxeWRnrN/\n/364urpi//796Nmz5yO9loeHx9OWSw1Mz55qfLkjFX8kXgQA3Lhdie/jCrFssifsmsprfC4RkSGr\n6UBWa6ff/fz8YGRkhMDAQISGhmLevHkICwtDTEyMxuNEd5qBAxgzZgzOnj2LsWPHYtu2bZg6daq2\nyqMGTiIW4d1Rrhjt6ySMXb9VijnrD+BCdqEOKyMiarhEaj3v9JGUlMQjdQO3PeYs/u/XU8K2uakM\nS97si873XFRHRNRY1JR7bD5DDd5oXydMGe0mrPCmKFVi0aYEJJ/hCm9ERPdiqJNeGNavLWa91hNS\nSXWyl1dUYdm3f+Pgca7wRkR0B0Od9IZ3j9ZYOLEPjGQSAEBllRqrwo/gr0Nc4Y2ICGCok57x6NQC\nH0/qB/N7VnhbF5WCHbHndFwZEZHuMdRJ7zzTvgk+edcLVvK7K7xt+eUkvvvtFFd4I6JGjaFOeql9\nayusnOqNZjZ3V3jbtvcsvtiRyhXeiKjRYqiT3mrdTI6VU7zRutndZjS/J1zE5z9whTciapxqDPX8\n/HysW7cOI0aMQI8ePeDh4YGRI0diw4YNXBaVGoRmNqZYOdULHezvrvAWd+wKVmw5jHKu8EZEjcxD\nQ/3777/H9OnTYWtri9DQUOzfvx8HDhzAypUrYWVlhalTp+K7776rz1qJHshKbowVk/vjmfZNhLGj\np3OxZFMiFKVc4Y2IGo+HdpSLjo7GkCFDanzyn3/+iaFDh2qlsEfFjnJ0R7myCqH/dwRHT+cKYx3s\nrfDR2/1gJTfWYWVERHXniTrK3Qn0efPmPXTHug50onsZyyRY8EZvDOxhL4xlXinEnPUHkHerVIeV\nERHVj1ovlEtPTxeWQyVq6KQSMT4c6w5/z7bC2NW8YszZEI+refx3TESGrdalV8ViMXx8fNCuXTsY\nG1efwhSJRPw8nRossViE4JGukJvKsG3vWQBA3q1SzF1/AB9N6of2ra1q2QMRkX6qNdRnzZoF4O4S\nqWzuQfpAJBJh3HNdIDc1wpZfTgIACorLMW/jASx+s6/GRXVERIai1tPvffr0QVlZGWJiYvDXX3+h\nqKgIffr0qY/aiJ7aSJ+OeO/V7hD/s8JbSVklFm9K1LiYjojIUNQa6l9//TXWr18POzs72Nvb48sv\nv8QXX3xRH7UR1Yln+zhidlAvYYW3CmUVln97CPHHruq4MiKiulXr6feffvoJ27Ztg4mJCQAgICAA\nI0aMQHBwsNaLI6or/d3sYGrcF5/832GUV1ShSqXG6u+PQlGmxLB+bXVdHhFRnaj1SF2tVgsXyAGA\nsbExZDKZVosi0gb3Ts2xbJInzE2r//2q1cCG7cexPeasjisjIqobtYZ637598f777yMmJgZ79+7F\ntGnT+Jk66a3O7WwR8m5/WFvc/UX1/349hbBfTvIiUCLSew/tKHeHWq3GDz/8gEOHDkGtVqNv374I\nCAiAVFrrmft6wY5y9CSybxRj0ZcJuH5PU5ph/dpi8khXSO5cVUdE1ADVlHu1hvrEiRPx7bffaqWw\nusBQpyd1s7AUi75KwOXcu01pvLu3xvQx7pBJuYAhETVMT9Qm9o6ysjJkZ2fXeVFEutbEyhQh73qh\nYxtrYSw+5SpWbDmEsopKHVZGRPRkag31/Px8+Pr6on///vD19YWvry8GDx5cH7URaV31Cm+e6Nah\nqTCWdOY6lmxKRDFXeCMiPVPr6ffU1FTY2treN25vb/+AR9c/nn6nulChrMKq8KM4dPKaMNbezgof\nTeqncVEdEZGuPdXp99mzZ8Pe3v6+P0SGxEgmwdzxvTDI4+6/7fPZhZi7IR7Xb5XosDIiokdXa6h3\n7twZu3btwvnz55GdnS38ITI0UokY0wPd8UL/dsLY1TwF5qyLx+XcIh1WRkT0aGq9L+348eM4fvz4\nfeMxMTFaKYhIl8RiESaN6Aa5mREi9qQDAG4UlmHuhuoV3jraW9eyByIi3ak11Bne1NiIRCK8NqwT\n5GYyfLM7DQBwW1GBBV8cxKKJfdD1novqiIgaklpPvxcUFGDhwoUICgpCfn4+5s2bh8LCwvqojUin\nXh7QAdMCNFd4W7IpEUdOXav5iUREOlJrqC9atAhdu3ZFQUEB5HI5mjdvLqyxTmTohvR2xJxxvSCV\nVP+oVFSqsGLLYcQlX9FxZURE96s11K9cuYLAwEBIJBIYGRlh+vTpyMnJqXXHKpUKixcvRmBgIIKC\ngpCVlaUxHxUVhVGjRiEgIAD79u0DAFy+fBmvvfYaXnvtNcyaNQtlZWVP9lUR1SFPVzsseasPTIwk\nAIAqlRqf/ZCE3xIu6LgyIiJNtYa6VCpFUdHdK38vXrwIiURS646jo6OhVCoRERGBmTNnIjQ0VJjL\ny8tDeHg4IiIisHnzZnz22WeoqKjA6tWrMXbsWHz//ffo3bs3tmzZ8oRfFlHd6u7cHMsme0J+zwpv\nX/yYim17Mx64EMyXO1Lx4ozd+HJHan2XSkSNWK2h/t577yEoKAg5OTkIDg7GmDFjMG3atFp3nJyc\nDG9vbwCAm5sb0tLShLnU1FS4u7tDJpNBLpfD0dER6enpyMzMxIABAwAA7u7uSEpKetKvi6jOdXK0\nRcgUL9jc04zmu99OI+yXUxrBXlpeKRzF/55wAaXlbDlLRPWj1qvfBwwYgK5du+L48eNQqVT4+OOP\n0axZs1p3XFxcDLlcLmxLJBKoVCqIxWIoFApYWFgIc+bm5iguLkanTp2wd+9eDB8+HHv37kVpaemD\ndk2kM21bWWLlVG8s+ioBufnVTWl27DuH4lIl3h3tBolYBGWlCncyXqUGlJUqmLIpHRHVg4eG+qef\nfopJkybB0tIStra28PHx0Zi/desWvv76a8yePfuBz5fL5VAoFML2nUB/0JxCoYClpSXmzp2LZcuW\n4ccff8TAgQNhY2PzSF8Ej+ipvr02wArhsRXIK6w+Cv/r0CVcybmOkf1sUVGp0njs8eMpMDOu/SMr\nIqKn9dBQ9/f3x5QpU9CsWTP06tULLVu2hEQiwdWrV3Ho0CHk5uZi/vz5D92xu7s7YmNj4e/vj5SU\nFLi4uAhzrq6uWLNmDSoqKlBeXo7MzEw4OTnht99+w/Tp09GuXTt8++236N+//yN9Eez9Trrg4V6B\nj75JREZWAQDgVFYpjE0q8N6r3QHcvZjUza07LM2NdFQlERmamg5ka13QJTExETExMcjKyoJIJIKD\ngwN8fHzQr1+/Gl9UrVZj6dKlSE+v7soVEhKCuLg4ODg4wNfXF9u2bUNkZCRUKhWCg4Ph5+eH1NRU\nfPTRRzAyMoKTkxOWLFlS60V5XNCFdKmkTIkVWw4j9dwNYcypjTXOXi4Qtr//2J+hTkR1pqbcqzXU\nGzqGOulahbIKq7cexd9pD25Kw1AnorpUU+7VeqHc/v378Z///AeFhYXCFb4ikQh79+6t2yqJ9JSR\nTIK543rhv1EpiDl6WdflEFEjVmuoL1++HPPmzUPHjh0hEonqoyYivSORiDEtoAfkpjL8FH9eY+7n\n+EwE+rlAIqn1DlIioqdSa6g/6Mp3IrqfWCzCWy93hUwqwY+xZ4XxiD0ZOHwqF++/2h0duMobEWlR\nrYcOHh4eCAkJwYEDB3DkyBHhDxHdTyQSYaRPx/vGz18txIdr9yPsl5MoV1bpoDIiagxqPVJPTa1u\nc3nq1CmN8fDwcO1URGSgVCo1fow9h4QTOZj6ihtcO9bexImI6HHUGuoMb6KnM+t1D2z55RRuFFR3\nSMy5ocCCLxLwbB9HvPHiM0I/eSKip1VrqB89ehTffPMNSktLoVKpoFKpkJOTg5iYmPqoj0jvdXdu\njg2zWiD899P49eAFoYXsX4cu4cipa5g80hWerna6LZKIDEKtn6kvWLAAQ4YMQVVVFV5//XU4Ojpi\n8ODB9VEbkcEwM5HhnRGuWDXVG21a3F0T4VZROUL+7wg+CTuMm4Vc64CInk6toW5iYoLRo0ejV69e\nsLS0xPLly3mhHNET6tTWFms/HISxz7pAKrl7i2jiiRxMWRWDP/+++MClXImIHsUjhXpBQQHatWuH\n48ePQyQScfU0ohrIpGLcaekgFlVva85LMGZoJ/znw0Fwcby7aJGirBLrtx3Hgi8SkJ1XXJ8lE5GB\nqDXUJ0yYgA8++AC+vr7YuXMnnn/+eXTt2rU+aiPSS6bGUjzn2Q4A4O/ZDqbGD750xbFl9TKuk4Z3\ng4nR3TUOTmTewHufxmJ7zFlUVqke+Fwiogd5pN7varUaIpEICoUCly5dQqdOnYRlVHWNvd/JEFy/\nVYKN248j6cx1jfH2dlZ4L6A7OrJpDRH9o6bcqzWZCwoKsGjRIgQFBaG8vBzh4eEoKiqq8yKJGrPm\nNmZY8lZfzHjNQ2Pxl/PZhZixdj+2/HwSZRWVOqyQiPRBraG+aNEidO3aFQUFBZDL5WjevDlmzZpV\nH7URNSoikQiD3O2xcbYvBnnYC+MqlRo79p3D+5/uw/GzeTqskIgaulpD/cqVKwgMDIREIoGRkRGm\nT5+OnJyc+qiNqFGykhtjxlgPLHmrL5rZmArjOTcVWPhlAv4beQzFJRU6rJCIGqpaQ10qlWqcbr94\n8SIkEkkNzyCiutCzcwtsmOWLF73b494FEvcczsK7q2JwMDWbt78RkYZaQ/29995DUFAQcnJyEBwc\njDFjxmDatGn1URtRo2dqLMWk4d2w6j1vOLS0EMZvFZUjlE1riOhfJEuXLl1a0wPkcjlKSkqQn5+P\noqIieHp6QqFQoHfv3vVUYs1ycnJgZ8cWm2TYmlqb4tk+jpBIxDh98SZU/xygX7lejL8OXYKFmRHa\nt7aC6N5DeiIySDXlXq29399++224uLjAx8dHuLWNiOqfTCrGmGdd0N+1FdZFpeDMpVsAgJKySmzY\nfhxxx648esTVAAAgAElEQVRg6ivd0bqZvJY9EZGhqjXURSIRQkJC6qMWInoEDv80rfk94QL+77dT\nKC2vXp89LfMm3vs0FmOedcGIQR0hlTSMXhJEVH9qPf2en5+PCxcuwNLSEgqFAkVFRSgqKoKFhUVN\nT6s3PP1OjZFIJIKzgw0GutsjO0+B7BsKANW3vx0/ewOHT15DxzbWaGJlWsueiEjfPNXp96KiImza\ntAk2NjYa41x6lUj3mtuYYfGbfbD/2FVs2nUCtxXVt7pdyL6NmWv34+WBHTF2qAtMjGr9USciA1Dr\nT/qff/6JxMREmJiY1Ec9RPSYRCIRBrrbo7tzM2z+KQ2xSVcAACo1sHPfOSSeyMbU0d3h5txMx5US\nkbbV+qGbg4MDCgsL66MWInoKVnJjfDjWAx+93Q/N72lac+1mCRZ+lYC1EcdQxKY1RAbtkc7JPffc\nc3BycoJMJgNQfWTw3XffabUwInoy7p2aY/0sX2z94zR+jj+PO/1poo9k4eiZXLwzohv6u9rxThYi\nA1RrqE+ePPm+Mf5nQNSwmRpL8fbL3TCge2usi0rBpWvVXSELisqx8ruj6PNMSwSPcuWFdEQG5pGW\nXm3IuPQqUc2UlSr8GHsWkXsyNNZnNzORYsLzXTC0b1uIxfxFnUhfPNXSq0Sk32RSMQL9XPDfGYPQ\nua2tMF5SVomNP6Zi/hcHceU6l1MmMgQMdaJGok0LC4RO8cLkka4wNb67KNPJ8zfx/mf7EBWteSRP\nRPqHoU7UiIjFIjzfvx02zBqMnp1bCOPKShXCfz+N6WvikJF1S4cVEtHTYKgTNULNbEyx+M0+mPW6\nB6zkRsL4xZzbmPXf/dj8UxrKyit1WCERPQmthbpKpcLixYsRGBiIoKAgZGVlacxHRUVh1KhRCAgI\nwL59+wAA2dnZeP311/H6669jypQpKCsr01Z5RI2eSCTCgB722Dh7MHx7thHGVWpgV1wmpn4ai2Pp\n13VYIRE9Lq2FenR0NJRKJSIiIjBz5kyEhoYKc3l5eQgPD0dERAQ2b96Mzz77DBUVFQgLC8Pzzz+P\nrVu3omPHjti+fbu2yiOif1iaG2H6GHd8NKkfmtuaCeO5+SVYvCkRa/6XzKY1RHpCa6GenJwMb29v\nAICbmxvS0tKEudTUVLi7u0Mmk0Eul8PR0RHp6eno0qWL0L2uuLhYaHZDRNrn7tIcG2b64OUBHXDv\nHW4xRy/j3ZUxiE+5Cj2/A5bI4Gkt1IuLiyGX313XWSKRQKWqvrJWoVBorPJmbm6O4uJitGjRAlu3\nbsULL7yAAwcOYOjQodoqj4gewMRYirde7orV7w9A21aWwnhBcTlWhR/F8m8P40ZBqQ4rJKKaaG3p\nJrlcDoVCIWyrVCqIxeIHzt0J+UWLFmHlypXo378/4uLiMGfOHHz11Ve1vlZSUlLdfwFEjdzrAyyQ\ncBqIS7uNO3e6HT51DSkZufDrYQWPjuYQs7skUYOitVB3d3dHbGws/P39kZKSAhcXF2HO1dUVa9as\nQUVFBcrLy5GZmQlnZ2dYWVkJR/fNmjXD7du3H+m12FGOSDv69AZG5xZh/bYUnLqQDwCoqFTj1yMF\nuHBDjKmvdEebFha17IWI6lJNB7JaaxOrVquxdOlSpKenAwBCQkIQFxcHBwcH+Pr6Ytu2bYiMjIRK\npUJwcDD8/PyQmZmJjz/+GCqVCmq1GgsXLkSnTp1qfB22iSXSPpVKjT/+voiwX06h9J5b3aQSMQL9\nnDHSxwkyKe+QJaoPNeUee78T0SO7UVCKL35MxeFT1zTG27ayxHuvdoezg42OKiNqPNj7nYjqRFNr\nUyyc2Buzg3rCWm4sjN9pWvPNbjatIdIlhjoRPRaRSATv7q2xYbYvBvfSbFqze38mpnwai2Q2rSHS\nCYY6ET0RS3MjfBDojo8n9UOLe5rWXM8vwZJ/mtbcVrBpDVF9YqgT0VPp4dIc62f6YPjABzStWbUX\n+49dYdMaonrCUCeip2ZiLMWbL93ftKawuAKrtyZh2beHkHeLTWuItI2hTkR1xtnBBmumD0SQf2eN\nW9yOnMrFlNUx+PXgBahUPGon0haGOhHVKalEjFeHOOO/MwbhmfZNhPHS8kp8uSMVczccwOXcIh1W\nSGS4GOpEpBX2zS3wSXB/vDvaDWYmd5tXnr6Yj/c/24eIPelQVqp0WCGR4WGoE5HWiMUi+Pdri42z\nfdHnmZbCeGWVCt//cQbT1+xD+qV8HVZIZFgY6kSkdU2sTLHgjd6YM06zac2la0WYtS4eX+86odF+\nloieDEOdiOqFSCSCl1trbJzjiyG9HIRxtRr4Kf48pq6OQfIZNq0hehoMdSKqVxZmRpgW2APL3umH\nlk3uaVpzqxRLvk7E5z8kobC4XIcVEukvhjoR6UR35+ZYN9MHIwZ11GhaE5t0Be+uisG+ZDatIXpc\nDHUi0hkTIykmvvgMPp02AO3s7jatua2owGffJ+HjzYdw/VaJDisk0i8MdSLSOac2Nvj8g4EY95xm\n05qjp3MxdXUMfjlwnk1riB4BQ52IGgSpRIxXBjtj3UyffzWtqcJXO09gzvp4ZF27rcMKiRo+hjoR\nNSitm8nxSXB/TPlX05ozl25h2udx+N9fbFpD9DAMdSJqcMRiEYY9pGnND3+ewQdr9uEMm9YQ3Yeh\nTkQN1p2mNXPH94K1xd2mNVnXijB7XTw2sWkNkQaGOhE1aCKRCP1d7fDFbF/49dZsWvNz/HlMWR2D\npDO5OqyQqOFgqBORXpCbGeH9gB5YPtlTo2lN3q1SLP36b3z2PZvWEDHUiUivuDk1w7qZPhj5r6Y1\n+5L/aVqTdJlNa6jRYqgTkd4xMZLijRefwWfTBqK9nZUwfltRgc9+SMZH3/yN6/lsWkOND0OdiPRW\nxzbW+OyDARj/fBcY3dO0JunMdUxZHYOf4jNRxaY11Igw1IlIr0klYoz2dcK6mT7o2uFu05qyiip8\nvSsNc9bH4xKb1lAjwVAnIoNg10yOFZP7Y+orbjC/p2lN+qVb+ODzffjhzzNQVlbpsEIi7WOoE5HB\nEItFGNq3LTbM9kW/bq2E8coqNf73VzqmfR6HMxdrb1rz5Y5UvDhjN77ckarNconqHEOdiAxOEytT\nzJ/QG/PG94LNPU1rLucWYfb6eHy1IxUlZcoHPre0vBK/JVwAAPyecIHNbUivMNSJyGB5utph42xf\nPNvHURhTq4FfDl7AlNWxOHr6/qY1ykoV7twRp1KDfeZJrzDUicigyc2M8N6r3bEi2BOtmpgL4zcK\nSvHRN3/j061sWkOGQ1r7Q56MSqXC0qVLkZGRAZlMhhUrVsDB4W6Lx6ioKERGRkIqlSI4OBiDBg3C\nJ598gtOnTwMA8vLyYGVlhcjISG2VSESNiGvHZlg3ywf/+/MMdsZlCuuzxx27guT063h7eFcMcrfX\ncZVET0droR4dHQ2lUomIiAgcP34coaGh2LhxI4DqwA4PD8eOHTtQXl6OMWPGwNPTE/PnzwcAVFZW\nYuzYsVi+fLm2yiOiRshYJsGEF56BV/fWWBeVgvNXCwEARSUV+PyHZOxLuoLxz3XWcZVET05rp9+T\nk5Ph7e0NAHBzc0NaWpowl5qaCnd3d8hkMsjlcjg6OiI9PV2YDw8Ph5eXF5ycnLRVHhE1Yh3trfH5\ntAGY8K+mNcnp1zF7fbwOKyN6OloL9eLiYsjlcmFbIpFApaq+4EShUMDCwkKYMzc3R3FxMQCgoqIC\nkZGRmDhxorZKIyKCRCLGqH+a1nTr0FQYL1dqXhjHPvKkT7R2+l0ul0OhUAjbKpUKYrH4gXMKhQKW\nlpYAgMTERPTu3VvjF4LaJCUl1VHVRNQYjextjLZNbPDXsQKUKzVDfMbn0RjkagUXexOIRaKH7IGo\nYdBaqLu7uyM2Nhb+/v5ISUmBi4uLMOfq6oo1a9agoqIC5eXlyMzMFE61JyQkYMCAAY/1Wh4eHnVa\nOxE1Pj17AqP9y7AuKkXjVrfcwkpExt+EY0sLvDrEGf3dWkMiZriT7tR0IKu10+9+fn4wMjJCYGAg\nQkNDMW/ePISFhSEmJgZNmzbFuHHjMHbsWIwfPx4ffvghjIyMAAAXL16EvT2vQCWi+mdraYLpY9wf\nOHfpWhFWb03ClFV7sfdIFiqreP86NTwitZ5/YJSUlMQjdSKqM7cVFXht8e/CdnenZkg5m3ff41rY\nmmG0rxMG92oDmVRSnyVSI1dT7rH5DBFRDWYF9cR/pg/U6CUPALn5Jdiw/TgmfRKNn+PPo1zJxWJI\n9xjqRES16GBvjfkTemP9TB8M6NEa936kfqOwDJt2ncBbK/ZgR+w59oonnWKoExE9IsdWlpj1ek9s\nnDMYg3u1gfiedC8oKseWX07izeV7EBmdDkXpgxeMIdImhjoR0T1kUjHu3LkmFlVv/1vrZnJ8EOiO\nr+YOxrB+bSGV3A33opIKbP39DN5c/he2/nEaRSUV9VU6EUOdiOhepsZSPOfZDgDg79kOpsYPv/O3\nZRNzTBnthq/n++FF7/Ya3ekUZZWI3JOBN5f/hbBfTqKgiIvGkPbx6nciojpy63YZdsVl4reECyir\n0LxwzkgmwbC+jhjp0xFNrEx1VCEZgppyj6FORFTHCovL8XP8efx84DxKyjQvnJNKxPDr7YDRvk5o\nbmumowpJnzHUiYh0oLhUiV8PnMfu/ZkoKtG8cE4iFsHHow1eGewEu2aP3habiKFORKRDJWVK/JF4\nETv3ZaKgWPOzdbEI8O5uj1eHOMGhpaVuCiS9wlAnImoAyioq8dehS9gRew43C8s05kQioF+3VggY\n4oL2ra10VCHpg5pyT2sLuhARkSYTIyle8u4A/35tEX3kMrbvzcD1W6UAALUaSEjNQUJqDnp3aYkA\nP2c4O9jouGLSNwx1IqJ6JpNK4N+vLfx6O2Bf0mVE7T2LnBt3l6M+fOoaDp+6hh7OzRDg54Jn2jfR\nYbWkTxjqREQ6IpWIMaS3I3w82iD+eDaiojNwObdImD+WkYdjGXl4pn0TBPo5w82pGURc051qwM/U\niYgaCJVKjcS0HETtycD57ML75l0cbRAwxBk9O7dguDdivFCOiEiPqNVqHDmdi8g96cjIKrhvvn1r\nKwQMcUbfrq00+s9T48AL5YiI9IhIJELvLi3Rq3MLpGTkITI6AyfP3xTmz18tRMj/HYFDSwu8OtgZ\nXt1bQ8JwJ/BInYhIL6Rl3kBkdAZSMvLum7Nrao5XBjtjkIc9pBIu6WHoePqdiMhAnLmUj6joDBw5\nlXvfXHNbM4z2dcKQXm0gk0p0UB3VB4Y6EZGBybxSgMjoDCSeyLlvromVCUb6dMTQvm1hLGO4GxqG\nOhGRgbp07Ta2RZ9FfMoVqP71v7m1hTFGDOxQ6xKypF8Y6kREBu5qXjG27z2L2KTLqPpXuluYyfDy\ngA54was9zE1lOqqQ6gpDnYiokcjNL8H2mLOIPpyFyiqVxpy5iRQveLXHSwM6wNLcSEcV0tNiqBMR\nNTI3CkqxY985/Jl4ERWVmuFuYiTBc57tMHxQB9hYmOimQHpiDHUiokbqVlEZdu3LxG8JF1BWUaUx\nZySTYFhfR4z06YgmVqY6qpAeF0OdiKiRu62owE/7M/HzgfMoKavUmJNKxPDr7YBRvk5oYWumowrp\nUTHUiYgIAFBcqsSvB85j9/5MFJUoNeYkYhF8PNrglcFOsGsm11GFVBuGOhERaSgtr8TvCRewc18m\nCorLNebEIsC7uz1eGeIEx5aWOqqQHoahTkRED1SurMKff1/EjthzuFlYdt+8p2srvDrYGR3srXVQ\nHT0IF3QhIqIHMpZJ8JJ3B/j3a4voI5exPeYsrueXCPMJqTlISM1Bry4tEDDEGS6OtjqslmrDUCci\nIsikEvj3awu/3g7Yl3QF2/ZmIPuGQpg/cioXR07lortzMwQMcUbXDk11WC09jNZCXaVSYenSpcjI\nyIBMJsOKFSvg4OAgzEdFRSEyMhJSqRTBwcEYNGgQSkpKsHTpUly9ehVKpRILFy6Eq6urtkokIqJ/\nkUrEGNLbAT492+BAylVE7c1A1rUiYT4lIw8pGXl4pn0TBAxxRnfnZhCJuOxrQ6G1UI+OjoZSqURE\nRASOHz+O0NBQbNy4EQCQl5eH8PBw7NixA+Xl5RgzZgw8PT2xefNmuLi4YNWqVUhPT8eZM2cY6kRE\nOiARizDQ3R7e3Vvj77QcREZn4PzVQmH+5PmbWLwpES4ONnjVzxm9OrdguDcAWgv15ORkeHt7AwDc\n3NyQlpYmzKWmpsLd3R0ymQwymQyOjo5IT0/HwYMH4e/vjzfffBNyuRxLlizRVnlERPQIxGIRPF3t\n0K9bKxw9nYvIPRlIz7olzKdn3cKyzYfQ3s4Kr/o5o1/XVhCLGe66ItbWjouLiyGX373PUSKRQKWq\nblWoUChgYWEhzJmbm6O4uBi3bt1CUVERNm/eDB8fH6xcuVJb5RER0WMQiUTo1aUlVr/vjWXv9MMz\n7ZtozJ/PLkTo/x3B1E9jsS/5yn2LylD90NqRulwuh0Jx9yILlUoFsVj8wLk7IW9tbQ1fX18AgI+P\nD77++utHeq2kpKQ6rJyIiGrzSl9T9GrfDPvTbuP8tbv3uV/OLcJn3ydhy0/H4d3FAq7tzCDhkXu9\n0Vqou7u7IzY2Fv7+/khJSYGLi4sw5+rqijVr1qCiogLl5eXIzMyEs7Mz3N3dsW/fPnTp0gVHjhyB\nk5PTI70W71MnIqp/HgBG+QPpl/IRGZ2BI6dyhbn8okrsPnQLiWfLMdrXCUN6tYFMKtFdsQakpgNZ\nrTWfUavVWLp0KdLT0wEAISEhiIuLg4ODA3x9fbFt2zZERkZCpVIhODgYfn5+KCwsxMKFC5GXlweZ\nTIaVK1fCzs6uxtdh8xkiooYh80oBovZmICE15765JlYmGOnTEc/2cYSJEe+mfhrsKEdERPXm0rXb\n2BZ9FvEpV/Dvj9at5cYYMagDhvVrCzMTmW4K1HMMdSIiqnfZecXYtvcsYpMu33fhnIWZDC8P6IDn\nvdpDbspwfxwMdSIi0pnc/BL8GHMWew5nobJKpTFnZiLFi17t8dKADrA0N9JRhfqFoU5ERDp3o6AU\nO/edwx+JF1FRqRnuJkYSPOfZDsMHdoCNpYluCtQTDHUiImowbhWVYXdcJn49eAFlFVUac0ZSMYb2\na4uRgzqiqbWpjips2BjqRETU4NxWVOCn+Ez8En8eirJKjbk7PehH+XREyybmOqqwYWKoExFRg1Vc\nqsSvB89jd1wmikqUGnNisQg+HvZ4ZbAzWjeTP2QPd325IxW/HryA5/u3w+SRhrl2SE25p7U2sURE\nRI9CbipDwBAXbF74LN544RlYWxgLcyqVGnuPXMa7K/di9dajuJRz+6H7KS2vxG8JFwAAvydcQGl5\n5UMfa6jYAYCIiBoEU2MpRvp0xPNe7fDX35fwY+xZ3CwsAwCo1MD+Y1ex/9hV9OvWCgFDnNHB3lrj\n+cpKFe6ce1apq7dNjf/9KoaNoU5ERA2KsUyCF73bY1g/R+w9chnbYs7ien6JMJ94IgeJJ3LQs3ML\nBPg5o5OjrQ6rbVgY6kRE1CDJpBIM69cWQ3o7YF/SFWzbm4HsG3cXAzt6OhdHT+eiu1MzvOrnDMeW\nljqstmFgqBMRUYN250p4n55tcCDlKqL2ZiDrWpEwn3I2Dyln8+DiaKPDKhsGhjoREekFiViEge72\n8O7eGn+n5SAyOgPnrxYK8+mXbumwuoaBoU5ERHpFLBbB09UO/bq1wtHTuYjck4H0LAY6wFAnIiI9\nJRKJ0KtLS/Ts3ALHz+Zh6x9nGv3ROu9TJyIivSYSidDduTkWv9lX16XoHEOdiIjIQDDUiYiIDARD\nnYiIyEAw1ImIiAwEQ52IiMhAMNSJiMggyKRiiETVfxeLqrcbm8b3FRMRkUEyNZbiOc92AAB/z3Yw\nNW58rVga31dMREQGa/JIV0we6arrMnSGR+pEREQGgqFORERkIBjqREREBoKhTkREZCAY6kRERAaC\noU5ERGQgGOpEREQGgqFORERkILTWfEalUmHp0qXIyMiATCbDihUr4ODgIMxHRUUhMjISUqkUwcHB\nGDRoEAoKCjB06FA4OzsDAPz8/DBu3DhtlUhERGRQtBbq0dHRUCqViIiIwPHjxxEaGoqNGzcCAPLy\n8hAeHo4dO3agvLwcY8aMgaenJ06dOoUXX3wRCxcu1FZZREREBktrp9+Tk5Ph7e0NAHBzc0NaWpow\nl5qaCnd3d8hkMsjlcjg6OiI9PR0nT55EWloagoKCMG3aNOTl5WmrPCIiIoOjtVAvLi6GXC4XtiUS\nCVQqFQBAoVDAwsJCmDM3N0dxcTHat2+PadOmITw8HEOGDMGyZcu0VR4REZHB0drpd7lcDoVCIWyr\nVCqIxeIHzikUClhaWsLV1RWmpqYAgCFDhuC///3vI71WUlJSHVZORESkn7QW6u7u7oiNjYW/vz9S\nUlLg4uIizLm6umLNmjWoqKhAeXk5MjMz4eTkhDlz5uDZZ5+Fv78/EhMT0bVr11pfx8PDQ1tfAhER\nkV4RqdVqtTZ2rFarsXTpUqSnpwMAQkJCEBcXBwcHB/j6+mLbtm2IjIyESqVCcHAw/Pz8cOXKFcyf\nPx9qtRrm5uZYvnw5mjZtqo3yiIiIDI7WQp2IiIjqF5vPEBERGQiGOhERkYFgqBMRERkIrV39TjU7\nfvw4Pv30U4SHh+PSpUuYO3cuxGIxnJycsGTJEohEIqxfvx5xcXGQSCSYP38+XF1dH/pYqj9KpRLz\n589HdnY2KioqEBwcjA4dOvA91CM3b97EyJEjERYWBrFYzPdOTyiVSsydOxdXr16FRCLBsmXLIJFI\n+P7dS031btOmTeoXXnhBHRAQoFar1ep33nlHffjwYbVarVYvXrxYvWfPHnVaWpp63LhxarVarc7O\nzlaPGjXqoY+l+vXjjz+qP/nkE7VarVYXFBSoBw4cqJ48eTLfQz1RUVGhfvfdd9VDhw5VZ2Zm8udP\nj+zZs0c9bdo0tVqtVh88eFA9depU/uz9C0+/64CjoyPWr18P9T83Hpw6dQq9evUCAAwYMAAJCQlI\nTk5G//79AQCtWrVCVVUV8vPzH/hYql/Dhg3D+++/D6C6qZJUKuV7qEdWrVqFMWPGoFmzZgD486dP\n2rVrh6qqKqjVahQVFUEmk+HkyZN8/+7BUNeBZ599FhKJRNhW33NXobm5OYqKilBcXPzAVrr3PtbM\nzAxFRUX1UzQJzMzMhPdj2rRp+OCDD4QWyADfw4Zsx44dsLW1hZeXF4Dqnz3+/OkPMzMzXL16FcOG\nDcPixYsRFBTE9+9fGOoNwJ32uUB1z3xLS8sHttK1sLDQeOyd9rpU/3JycjB+/HgMHz4cL7zwAt9D\nPbFjxw4kJCQgKCgIZ86cwdy5c3Hr1i1hnu9dwxYWFgZvb2/8+eef2L17N+bMmYPKykphnu8fQ71B\n6Ny5Mw4fPgwA2L9/P3r27Al3d3ccOHAAarUa2dnZUKvVsLGxeeBjqX7duHEDEydOxKxZszBy5EgA\nfA/1xdatWxEeHo7w8HB06tQJK1euhJeXF987PWFlZQVzc3MAgKWlJSorK9GlSxe+f/fg1e86dOeq\ny7lz52LRokVQKpXo0KEDhg0bBpFIhJ49eyIgIAAqlQqLFy9+6GOpfn355ZcoKirChg0bsGHDBgDA\nggULsGLFCr6HekYkEvHnT49MmDAB8+fPx2uvvQalUokZM2bgmWee4ft3D7aJJSIiMhA8/U5ERGQg\nGOpEREQGgqFORERkIBjqREREBoKhTkREZCAY6kRERAaCoU5Ej2THjh2YN29ene0vLS0NCxcurLP9\nERFDnYgeUV0vUdm1a1csX768TvdJ1NixoxyRnrt27RpmzpyJ0tJSiMViLFy4ENnZ2QgLC0NZWRnK\nysqwYsUK9OzZE0FBQejSpQsSEhJQXl6OhQsX4rvvvkNmZibGjx+PCRMmYN26dbh48SIuX76MgoIC\nBAQE4M0339RYDCM1NRWhoaEoKyuDjY0NPvroI9jb2z+0xp9//hmbN2+GWCyGvb09Pv30Uxw7dgzr\n16/Ht99+i9GjRwu/NFy+fBkjRozAwoULsWnTJvzxxx+oqqqCl5cXZs2apfXvJ5E+Y6gT6bnt27fD\nx8cHb775Jg4fPowjR47g4MGD+Oqrr2BtbY3t27fjm2++Efpci0Qi/Pzzz1i/fj2WL1+On3/+GTdv\n3sTw4cMxYcIEAMC5c+cQERGBqqoqjBw5Ev369RNCV6lUCoHbsmVLxMfHY9GiRdiyZctDa1y7di2i\noqJga2uL//znPzh//rywP5lMht27dwMAUlJSMG/ePEydOhX79+/HyZMnsX37dgDArFmz8NNPP+Gl\nl17S1reSSO8x1In0nKenJ9577z2cOnUKgwYNwrhx4xAYGIiYmBhcuHABR44c0Vjqd8CAAQAAOzs7\nuLm5wdjYGHZ2drh9+zaA6tB//vnnYWpqCgDw9fXF33//DRsbGwAQjuInT54s7PPeFbEexMfHB2PG\njMHgwYMxdOhQdOrUCYcOHdJ4TG5uLmbNmoV169bB2toaiYmJSE1NFRbNKS8vr/FsABEx1In0nru7\nO3799Vfs27cPv/32G6KiopCXl4fhw4ejd+/e6NSpE7Zu3So8XiaTCX+XSh/8X8C9vwRUVVXdt92m\nTRvs2rULAKBSqXDjxo0aa1ywYAFGjx6NuLg4zJo1C1OnTkXLli2F+fLyckyZMgXvv/8+OnXqJOz3\nzkcCAFBUVKRRBxHdjxfKEem51atXY/fu3Rg+fDgWLVokHJm/88476NOnD+Li4qBSqR55f2q1Gnv2\n7IFSqURhYSH27dsHLy8v4TP19u3bo7CwEEePHgVQffp/xowZD91fZWUlhg4dChsbG0yaNAkvv/wy\nTp8+rfGY+fPno1evXnjxxReFsb59+2L37t0oKSlBZWUl3n33Xfz111+P860hanR4pE6k54KCgjBj\nxs91axsAAAEFSURBVAzs3LkTYrEYa9asQXR0NIYNGwZTU1P06tULOTk59z3v31ez37ttYmKCsWPH\nori4GO+88w46dOiA1NRUAICRkRHWrl2LFStWoLy8HBYWFggNDX1ofVKpFO+//z7eeOMNmJiYwMrK\nCqGhobh48SJEIhGOHTuGX3/9Fd26dcOIESOgVqvh5OSE1atX48yZM3j11VdRVVWFAQMGYPjw4XX0\nXSMyTFx6lYg0rF+/HgAwdepUHVdCRI+LR+pEVCdWrVqFhISE+8a7deuGZcuW6aAiosaHR+pEREQG\nghfKERERGQiGOhERkYFgqBMRERkIhjoREZGBYKgTEREZCIY6ERGRgfh/JbJiAiYzhKMAAAAASUVO\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112b52e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.pointplot(x='sample_size', y='error', data=melted, markers=[''], ci=99).set_title('Error Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2 (Na¨ıve Bayes; 20 points). Download the “20 Newsgroups data set” news.mat from\n",
    "Courseworks. The training feature vectors/labels and test feature vectors/labels are stored as\n",
    "data/labels and testdata/testlabels. Each data point corresponds to a message posted to one\n",
    "of 20 different newsgroups (i.e., message boards). The representation of a message is a (sparse)\n",
    "binary vector in X := {0, 1}\n",
    "d\n",
    "(for d := 61188) that indicates the words that are present in the\n",
    "message. \n",
    "\n",
    "If the j-th entry in the vector is 1, it means the message contains the word that is given\n",
    "on the j-th line of the text file news.vocab. The class labels are Y := {1, 2, . . . , 20}, where the\n",
    "mapping from classes to newsgroups is in the file news.groups (which we won’t actually need).\n",
    "In this problem, you’ll develop a classifier based on a Na¨ıve Bayes generative model. Here, we\n",
    "use class conditional distributions of the form\n",
    "$$Pµ(x) = Y\n",
    "d\n",
    "j=1\n",
    "µ\n",
    "xj\n",
    "j\n",
    "(1 − µj )\n",
    "1−xj\n",
    "for x = (x1, x2, . . . , xd) ∈ X $$\n",
    "for some parameter vector µ = (µ1, µ2, . . . , µd) ∈ [0, 1]d\n",
    ". \n",
    "\n",
    "Since there are 20 classes, the generative\n",
    "model is actually parameterized by 20 such vectors, µy = (µy,1, µy,2, . . . , µy,d) for each y ∈ Y, as\n",
    "well as the class prior parameters, πy for each y ∈ Y. The class prior parameters, of course, must\n",
    "satisfy πy ∈ [0, 1] for each y ∈ Y and P\n",
    "y∈Y πy = 1.\n",
    "\n",
    "\n",
    "(a) Give the formula for the MLE of the parameter µy,j based on training data {(xi\n",
    ", yi)}\n",
    "n\n",
    "i=1.\n",
    "(Remember, each unlabeled point is a vector: xi = (xi,1, xi,2, . . . , xi,d) ∈ {0, 1}\n",
    "d\n",
    ".)\n",
    "\n",
    "(b) It turns out the MLE is not a good estimator for the class conditional parameters, especially if\n",
    "the estimate turns out to be zero or one. An alternative to the MLE is the following estimator\n",
    "based on a technique called Laplace smoothing:\n",
    "$$µˆy,j :=\n",
    "1 + Pn\n",
    "i=1 1{yi = y}xi,j\n",
    "2 + Pn\n",
    "i=1 1{yi = y}$$\n",
    "This ensures that ˆµy,j ∈ (0, 1)—never zero nor one.\n",
    "Write a function that takes as input a (sparse) matrix of training feature vectors X and a\n",
    "vector of labels Y (as data and labels above), and returns the parameters params of the\n",
    "classifier based on this Na¨ıve Bayes generative model. Use the Laplace smoothing estimator\n",
    "for the class conditional distribution parameters, and use MLE for the class prior parameters.\n",
    "Naturally, you should not use or look at any existing implementation (e.g., such as those that\n",
    "may be provided as library functions).\n",
    "Also, write a function that takes as input the parameters of the above classifier params, and\n",
    "a matrix of test feature vectors test. The function should output a vector of predictions\n",
    "preds for all test feature vectors.\n",
    "Train and evaluate a classifier using this code and the data from news.mat. Submit your\n",
    "source code, and report the training and test error rates in your write-up. Save the learned\n",
    "classifier for part (c)!\n",
    "\n",
    "(c) The classifier you learn should have the following form: x 7→ arg maxy∈Y αy,0 +\n",
    "Pd\n",
    "j=1 αy,jxj\n",
    "for some real numbers αy,0, αy,1, . . . , αy,d for each y ∈ Y. Compute the values of these αy,j ’s\n",
    "from (the parameters of) your learned classifier from part (b).\n",
    "For each class y ∈ Y, report the vocabulary words whose indices j ∈ {1, 2, . . . , d} correspond\n",
    "to the 20 largest (i.e., most positive) αy,j value. Don’t report the j’s, but rather the actual\n",
    "vocabulary words (from news.vocab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/numpy/core/fromnumeric.py:2641: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  VisibleDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "news = loadmat('./homework1/data/news.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 7505 x 61188 sparse matrix\n",
    "p2testdata = news['testdata'].toarray()\n",
    "# 7505 x 1 array\n",
    "p2testlabels = news['testlabels']\n",
    "\n",
    "# 11269 x 61188 sparse matrix\n",
    "p2trainingdata = news['data'].toarray()\n",
    "# 11269 x 1 array\n",
    "p2traininglabels = news['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v = p2traininglabels\n",
    "breakpoints = np.where(v[:-1] != v[1:])[0]\n",
    "Boards = np.array(np.split(p2trainingdata, breakpoints))\n",
    "# mus = np.array([board.mean(axis=1) for board in Boards])\n",
    "# smoothed_mus = np.array([(board.sum(axis=0) + 1) / (board.shape[0] + 2) for board in Boards])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit(X, Y, laplace=False):\n",
    "    \"\"\" Create parameter weightings. \n",
    "    Args:\n",
    "        X: matrix of training feature vectors\n",
    "        Y: vector of labels\n",
    "    \n",
    "    TODO: Assumes labels and training data are sorted!\n",
    "    \"\"\"\n",
    "    breakpoints = np.where(Y[:-1] != Y[1:])[0]\n",
    "    Boards = np.array(np.split(X, breakpoints))\n",
    "    if laplace:\n",
    "        mus = np.array([(board.sum(axis=0) + 1) / (board.shape[0] + 2) for board in Boards])\n",
    "    else: \n",
    "        mus = np.array([board.mean(axis=1) for board in Boards])\n",
    "    \n",
    "    return mus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loglikelihoods(Mu, x):\n",
    "    \"\"\" Computes the loglikelihod of a vector for each class \n",
    "    \n",
    "    Args:\n",
    "        Mu: 20 x d matrix of estimated parameter values\n",
    "        x: 1 x d vector of test values\n",
    "        \n",
    "    Returns:\n",
    "        likelihoods: 20 x d matrix of likelihood values\n",
    "    \"\"\"\n",
    "    return x.dot(np.log(Mu)) + (1-x).dot(np.log(1-Mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def posteriors(loglikelihoods, priors):\n",
    "    \"\"\" Computes the posterior \n",
    "    Args:\n",
    "        loglikelihoods: Matrix of loglikelihood values\n",
    "        priors: 1 x d dimensional vector of priors\n",
    "    \n",
    "    Returns:\n",
    "        posteriors: Matrix of posterior values\n",
    "        \n",
    "    TODO: clean up the transpose\n",
    "    \"\"\"\n",
    "    return loglikelihoods.T + np.log(priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(params, testdata, priors):\n",
    "    likelihoods = np.apply_along_axis(loglikelihoods, 1, params, testdata)\n",
    "    posts = posteriors(likelihoods, priors)\n",
    "    preds = posts.argmax(axis=1)\n",
    "    \n",
    "    return preds + 1 # adjust the indicies to match the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Using the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "priors = np.array([(board.shape[0] / Boards.shape[0]) / 100 for board in Boards])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = fit(p2trainingdata, p2traininglabels, laplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_preds = predict(params, p2trainingdata, priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_errors = compute_error_rate(training_preds, [l[0] for l in p2traininglabels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21705563936462863"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_preds = predict(params, p2testdata, priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_errors = compute_error_rate(test_preds, [l[0] for l in p2testlabels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3786808794137242"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphas = ((np.log(params) - np.log(1-params)).T + np.log(priors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 61188)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.60298797, -7.60298797, -7.60298797, ...,  1.05570479,\n",
       "        1.08535348,  2.42307114])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort((alphas.T[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ttia = np.argsort(alphas.T)[:, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ttim = np.argsort(-params)[:20, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = open('./homework1/data/vocab.txt').readlines()\n",
    "vocab = np.array([w[:-1] for w in vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_words = []\n",
    "for arr in ttia:\n",
    "    top_word = list(vocab[arr])\n",
    "    top_words.append(top_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twa = np.apply_along_axis(lambda x: vocab[x], 0, ttia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twm = np.apply_along_axis(lambda x: vocab[x], 0, ttim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tdf = pd.DataFrame(twa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "(Cost-sensitive classification; 10 points). Suppose you face a binary classification problem\n",
    "with input space X = R and output space Y = {0, 1}, where it is c times as bad to commit a\n",
    "“false positive” as it is to commit a “false negative” (for some real number c ≥ 1). To make this\n",
    "concrete, let’s say that if your classifier predicts 1 but the correct label is 0, you incur a penalty of\n",
    "✩c; if your classifier predicts 0 but the correct label is 1, you incur a penalty of ✩1. (And you incur\n",
    "no penalty if your classifier predicts the correct label.)\n",
    "Assume the distribution you care about has a class prior with π0 = 2/3 and π1 = 1/3, and the\n",
    "class conditional densities are N(0, 1) for class 0, and N(1, 1/4) for class 1. Let f\n",
    "?\n",
    ": R → {0, 1} be\n",
    "the classifier with the smallest expected penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 \n",
    "(Probability; 10 points). Suppose you have an urn containing 100 colored balls. Each\n",
    "ball is painted with one of five possible colors from the color set C := {red, orange, yellow, green, blue}.\n",
    "For each c ∈ C, let nc denote the number of balls in the urn with color c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability that two randomly selected balls have different colors is one minus the probability that any two colored balls are selected.  Since the probability of selecting two balls of the same color simultaneously is the $\\big(\\frac{n_{color}}{100})^2$, the probability of choosing two similarly colored balls is the probability of selecting two of any of the colors, which is just the sum of the probabilities of any one of the colors.  The probability of not getting any two colors is the complement of that, or\n",
    "\n",
    "$$ P(difference) = 1 - \\frac{1}{100^2} (n_r^2 + n_o^2 + n_y^2 + n_g^2 + n_b^2)  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jetsam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def group(lst, n):\n",
    "  for i in range(0, len(lst), n):\n",
    "    val = lst[i:i+n]\n",
    "    if len(val) == n:\n",
    "      yield tuple(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dists = np.apply_along_axis(compute_distances, 1, testdata, trainingdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_datasets = {}\n",
    "for n in [1000, 2000, 4000, 8000]:\n",
    "    samples = []\n",
    "    for sample in range(10):\n",
    "        sel = random.sample(xrange(60000), n) \n",
    "        data = ocr['data'][sel]\n",
    "        labels = ocr['labels'][sel]\n",
    "        samples.append({'data':data, 'labels':labels})\n",
    "    training_datasets[n] = samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn(ix, testdata, testlabels, training_data, training_labels):\n",
    "    distances = compute_distances(testdata[ix], training_data)\n",
    "    minima = np.argmin(distances)\n",
    "    predicted_label = predict(minima, training_labels)\n",
    "    actual_label = testlabels[ix][0]\n",
    "    check_prediction(predicted_label, actual_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
