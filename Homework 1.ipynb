{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1:\n",
    "Write a function that implements the 1-nearest neighbor classifier with Euclidean distance.\n",
    "Your function should take as input a matrix of training feature vectors X and a vector of the\n",
    "corresponding labels Y, as well as a matrix of test feature vectors test. The output should be a\n",
    "vector of predicted labels preds for all the test points. Naturally, you should not use (or look at the\n",
    "source code for) any library functions for computing Euclidean distances, nearest neighbor queries,\n",
    "and so on. If in doubt about what is okay to use, just ask.\n",
    "\n",
    "Instead of using your 1-NN code directly with data and labels as the training data, do the\n",
    "following. For each value n ∈ {1000, 2000, 4000, 8000},\n",
    " - Draw n random points from data, together with their corresponding labels. In Python, use sel = random.sample(xrange(60000),n)\n",
    "(after import random), ocr['data'][sel], and ocr['labels'][sel].\n",
    " - Use these n points as the training data and testdata as the test points, and compute the\n",
    "test error rate of the 1-NN classifier.\n",
    "\n",
    "A plot of the error rate (on the y-axis) as a function of n (on the x-axis) is called a learning curve.\n",
    "We get an estimate of this curve by using the test error rate in place of the (true) error rate.\n",
    "Since the above process involves some randomness, you should repeat it independently several\n",
    "times (say, at least ten times). Produce an estimate of the learning curve plot using the average of\n",
    "these test error rates (that is, averaging over the repeated trials). Add error bars to your plot that\n",
    "extend to one standard deviation above and below the means. Ensure the plot axes are properly\n",
    "labeled. Submit the plot and your source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, cdist, squareform, euclidean\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "ocr = loadmat('./data/ocr.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testdata = ocr['testdata']\n",
    "testlabels = ocr['testlabels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_image(X):\n",
    "    plt.imshow(X.reshape((28, 28)), cmap=cm.gray_r )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_distances(x, Y):\n",
    "    \"\"\" Computes the euclidian ditances between a test vector \n",
    "    and list of training vectors \n",
    "    \"\"\"\n",
    "    return np.sqrt(((x-Y)**2).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def euclidian_distance(x, y):\n",
    "    \"\"\" Computes the euclidian distance between two vectors \"\"\"\n",
    "    return np.sqrt(np.dot(x, x) - 2 * np.dot(x, y) + np.dot(y, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_training_data(sample_size, collection_size):\n",
    "    collection = []\n",
    "    for sample in range(collection_size):\n",
    "        sel = random.sample(xrange(60000), sample_size) \n",
    "        data = ocr['data'][sel]\n",
    "        labels = ocr['labels'][sel]\n",
    "        collection.append({'data':data, 'labels':labels})\n",
    "    \n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_error_rates(sample_size):\n",
    "    actual = [lab[0] for lab in testlabels]\n",
    "    error_rates = []\n",
    "    for training in generate_training_data(sample_size, 10):\n",
    "        trainingdata = training['data']\n",
    "        traininglabels = training['labels']\n",
    "        preds = nearest_neighbors(testdata, trainingdata, traininglabels)\n",
    "        error_rate = compute_error_rate(preds, actual)\n",
    "        error_rates.append(error_rate)\n",
    "    \n",
    "    return error_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error_rate(preds, actual):\n",
    "    zipped = zip(preds, actual)\n",
    "    errors = [tup for tup in zipped if tup[0] != tup[1]]\n",
    "    \n",
    "    return len(errors) / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nearest_neighbors(testdata, trainingdata, traininglabels):\n",
    "    dists = cdist(testdata, trainingdata, 'euclidean')\n",
    "    nnixs = np.argmin(dists, axis=1)\n",
    "    preds = [traininglabels[ix][0] for ix in nnixs]\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "onethousand_errors = compute_error_rates(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twothousand_errors = compute_error_rates(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([1000, 2000])\n",
    "y = np.array([onethousand_errors, twothousand_errors])\n",
    "e = np.array([1.5, 2.6, 3.7, 4.6, 5.5])\n",
    "\n",
    "plt.errorbar(x, y, e, linestyle='None', marker='^')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2 (Na¨ıve Bayes; 20 points). Download the “20 Newsgroups data set” news.mat from\n",
    "Courseworks. The training feature vectors/labels and test feature vectors/labels are stored as\n",
    "data/labels and testdata/testlabels. Each data point corresponds to a message posted to one\n",
    "of 20 different newsgroups (i.e., message boards). The representation of a message is a (sparse)\n",
    "binary vector in X := {0, 1}\n",
    "d\n",
    "(for d := 61188) that indicates the words that are present in the\n",
    "message. \n",
    "\n",
    "If the j-th entry in the vector is 1, it means the message contains the word that is given\n",
    "on the j-th line of the text file news.vocab. The class labels are Y := {1, 2, . . . , 20}, where the\n",
    "mapping from classes to newsgroups is in the file news.groups (which we won’t actually need).\n",
    "In this problem, you’ll develop a classifier based on a Na¨ıve Bayes generative model. Here, we\n",
    "use class conditional distributions of the form\n",
    "$$Pµ(x) = Y\n",
    "d\n",
    "j=1\n",
    "µ\n",
    "xj\n",
    "j\n",
    "(1 − µj )\n",
    "1−xj\n",
    "for x = (x1, x2, . . . , xd) ∈ X $$\n",
    "for some parameter vector µ = (µ1, µ2, . . . , µd) ∈ [0, 1]d\n",
    ". \n",
    "\n",
    "Since there are 20 classes, the generative\n",
    "model is actually parameterized by 20 such vectors, µy = (µy,1, µy,2, . . . , µy,d) for each y ∈ Y, as\n",
    "well as the class prior parameters, πy for each y ∈ Y. The class prior parameters, of course, must\n",
    "satisfy πy ∈ [0, 1] for each y ∈ Y and P\n",
    "y∈Y πy = 1.\n",
    "\n",
    "\n",
    "(a) Give the formula for the MLE of the parameter µy,j based on training data {(xi\n",
    ", yi)}\n",
    "n\n",
    "i=1.\n",
    "(Remember, each unlabeled point is a vector: xi = (xi,1, xi,2, . . . , xi,d) ∈ {0, 1}\n",
    "d\n",
    ".)\n",
    "\n",
    "(b) It turns out the MLE is not a good estimator for the class conditional parameters, especially if\n",
    "the estimate turns out to be zero or one. An alternative to the MLE is the following estimator\n",
    "based on a technique called Laplace smoothing:\n",
    "$$µˆy,j :=\n",
    "1 + Pn\n",
    "i=1 1{yi = y}xi,j\n",
    "2 + Pn\n",
    "i=1 1{yi = y}$$\n",
    "This ensures that ˆµy,j ∈ (0, 1)—never zero nor one.\n",
    "Write a function that takes as input a (sparse) matrix of training feature vectors X and a\n",
    "vector of labels Y (as data and labels above), and returns the parameters params of the\n",
    "classifier based on this Na¨ıve Bayes generative model. Use the Laplace smoothing estimator\n",
    "for the class conditional distribution parameters, and use MLE for the class prior parameters.\n",
    "Naturally, you should not use or look at any existing implementation (e.g., such as those that\n",
    "may be provided as library functions).\n",
    "Also, write a function that takes as input the parameters of the above classifier params, and\n",
    "a matrix of test feature vectors test. The function should output a vector of predictions\n",
    "preds for all test feature vectors.\n",
    "Train and evaluate a classifier using this code and the data from news.mat. Submit your\n",
    "source code, and report the training and test error rates in your write-up. Save the learned\n",
    "classifier for part (c)!\n",
    "\n",
    "(c) The classifier you learn should have the following form: x 7→ arg maxy∈Y αy,0 +\n",
    "Pd\n",
    "j=1 αy,jxj\n",
    "for some real numbers αy,0, αy,1, . . . , αy,d for each y ∈ Y. Compute the values of these αy,j ’s\n",
    "from (the parameters of) your learned classifier from part (b).\n",
    "For each class y ∈ Y, report the vocabulary words whose indices j ∈ {1, 2, . . . , d} correspond\n",
    "to the 20 largest (i.e., most positive) αy,j value. Don’t report the j’s, but rather the actual\n",
    "vocabulary words (from news.vocab)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jetsam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dists = np.apply_along_axis(compute_distances, 1, testdata[:1000], trainingdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_datasets = {}\n",
    "for n in [1000, 2000, 4000, 8000]:\n",
    "    samples = []\n",
    "    for sample in range(10):\n",
    "        sel = random.sample(xrange(60000), n) \n",
    "        data = ocr['data'][sel]\n",
    "        labels = ocr['labels'][sel]\n",
    "        samples.append({'data':data, 'labels':labels})\n",
    "    training_datasets[n] = samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn(ix, testdata, testlabels, training_data, training_labels):\n",
    "    distances = compute_distances(testdata[ix], training_data)\n",
    "    minima = np.argmin(distances)\n",
    "    predicted_label = predict(minima, training_labels)\n",
    "    actual_label = testlabels[ix][0]\n",
    "    check_prediction(predicted_label, actual_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
