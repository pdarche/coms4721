{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "spam = loadmat('./homework2/data/spam_fixed.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam_data = spam['data']\n",
    "spam_labels = spam['labels']\n",
    "spam_test_data = spam['testdata']\n",
    "spam_test_labels = spam['testlabels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_errors(predictions, labels):\n",
    "    \"\"\" Generates a list of indexes of misclassified \n",
    "    examples\n",
    "    \"\"\"\n",
    "    zipped = zip(predictions, labels)\n",
    "    errors = [ix for ix, tup in enumerate(zipped)\n",
    "              if tup[0] != tup[1]]\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_scikits_model(clf, test_data, test_labels):\n",
    "    preds = clf.predict(test_data)\n",
    "    errors = compute_errors(preds, [t[0] for t in test_labels])\n",
    "\n",
    "    return len(errors) / test_labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def combinations(x):\n",
    "    combos = np.array(list(itertools.combinations(x,2)))\n",
    "    x_prime = np.array([x1 * x2.T for x1, x2 in combos])\n",
    "    \n",
    "    return x_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_features(features):\n",
    "    original = features\n",
    "    squared = features ** 2\n",
    "    combos = combinations(features)\n",
    "    return np.concatenate([original, squared, combos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Averaged-Perceptron with 64 passes through the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(features,  weights):\n",
    "    \"\"\" \n",
    "    Predicts a label (1, -1) given a vector \n",
    "    of features and weights\n",
    "    \n",
    "    Args:\n",
    "        features:\n",
    "        weights:\n",
    "\n",
    "    Return:\n",
    "        prediction: int of -1 or 1\n",
    "    \"\"\"\n",
    "    prediction = np.dot(features, weights)\n",
    "    if prediction > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_weights(prediction, label, features, weights):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        prediction: int of predicted label (1 or -1)\n",
    "        label: the true label of the data point (1 or -1)\n",
    "        features: numpy array of feature values\n",
    "        weights: 1d numpy array of weights for the features\n",
    "    \n",
    "    Returns:\n",
    "        weights: \n",
    "    \"\"\"\n",
    "    if prediction != label:\n",
    "        weights = weights + (label * features)\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def perceptron_fit(examples):\n",
    "    \"\"\" \n",
    "    Generates a vector of weights\n",
    "    \n",
    "    Args:\n",
    "        examples: vector of feature, label tuples\n",
    "    \n",
    "    Returns:\n",
    "        weights: d-dimensional vector of weights\n",
    "    \"\"\"\n",
    "    weights = np.zeros(examples[0][0].shape)\n",
    "    for features, label in examples:\n",
    "        prediction = predict(features, weights)\n",
    "        weights = update_weights(prediction, label[0], \n",
    "                                 features, weights)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add trained bias to signature and prediction\n",
    "def test_perceptron_model(predict, testdata, testlabels, trained_weights):\n",
    "    \"\"\" Generates predictions from a trained weight vector \"\"\"\n",
    "    preds = [predict(features, trained_weights)\n",
    "             for features in testdata]\n",
    "    errors = compute_errors(preds, [t[0] for t in testlabels])\n",
    "    \n",
    "    return len(errors) / testlabels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def avg_perceptron_train(num_iterations, examples):\n",
    "    weights = np.zeros(examples[0][0].shape)\n",
    "    cweights = np.zeros(examples[0][0].shape)\n",
    "    bias = 0\n",
    "    cbias = 0\n",
    "    counter = 1\n",
    "    \n",
    "    for iteration in range(0, num_iterations):\n",
    "        np.random.shuffle(examples)\n",
    "        for features, label in examples:\n",
    "            if np.dot(features, weights) + bias <= 0:\n",
    "                # update the weights for this iteration\n",
    "                weights = weights + (label * features)\n",
    "                bias = bias + label\n",
    "                # update the cached weights\n",
    "                cweights = cweights + (label * counter * features)\n",
    "                cbias = bias + (label * counter)\n",
    "            counter += 1\n",
    "            \n",
    "    return (weights - ((1/counter) * cweights), bias - ((1/counter) * cbias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_perceptron_train(num_iterations, examples):\n",
    "    weights = np.zeros(examples[0][0].shape)\n",
    "    cweights = np.zeros(examples[0][0].shape)\n",
    "    bias = 0\n",
    "    counter = 1\n",
    "    \n",
    "    for iteration in range(0, num_iterations):\n",
    "        np.random.shuffle(examples)\n",
    "        for features, label in examples:\n",
    "            if np.dot(features, weights) <= 0:\n",
    "                # update the weights for this iteration\n",
    "                weights = weights + (label * features)\n",
    "                # update the cached weights\n",
    "                cweights = cweights + (label * counter * features)\n",
    "            counter += 1\n",
    "            \n",
    "    return weights - ((1/counter) * cweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_perceptron_test(features, weights, bias):\n",
    "    activation = np.dot(features, weights) + bias\n",
    "    if activation > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AveragedPerceptron(object):\n",
    "    def __init__(self):\n",
    "        self.num_iterations = 64\n",
    "    \n",
    "    def _prep_examples(self, X, y):\n",
    "        return zip(X, y)\n",
    "    \n",
    "    def fit(self, training_data, training_labels):\n",
    "        examples = self._prep_examples(training_data, training_labels)\n",
    "        weights = np.zeros(examples[0][0].shape)\n",
    "        cweights = np.zeros(examples[0][0].shape)\n",
    "        bias = 0\n",
    "        counter = 1\n",
    "\n",
    "        for iteration in range(0, self.num_iterations):\n",
    "            np.random.shuffle(examples)\n",
    "            for features, label in examples:\n",
    "                if np.dot(features, weights) <= 0:\n",
    "                    # update the weights for this iteration\n",
    "                    weights = weights + (label * features)\n",
    "                    # update the cached weights\n",
    "                    cweights = cweights + (label * counter * features)\n",
    "                counter += 1\n",
    "\n",
    "        self.model = weights - ((1/counter) * cweights)\n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        return np.array([self._predict_one(features) \n",
    "                         for features in test_data])\n",
    "    \n",
    "    def _predict_one(self, features):\n",
    "        activation = np.dot(features, self.model)\n",
    "        if activation > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Averaged-Perceptron with 64 passes through the data.\n",
    "# spam_examples = zip(spam_data, spam_labels)\n",
    "# trained_weights = avg_perceptron_train(64, spam_examples)\n",
    "# test_perceptron_model(predict, spam_test_data, spam_test_labels, trained_weights)\n",
    "clf = AveragedPerceptron()\n",
    "clf.fit(spam_data, spam_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14127604166666666"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scikits_model(clf, spam_test_data, spam_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Logistic regression model with MLE for parameter estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07747395833333333"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(spam_data, np.ravel(spam_labels))\n",
    "test_scikits_model(clf, spam_test_data, spam_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3  Generative model classifier where class conditional distributions are multivariate Gaussian distributions with shared covariance matrix for all classes. Use MLE for parameter estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12239583333333333"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(spam_data, np.ravel(spam_labels))\n",
    "test_scikits_model(lda, spam_test_data, spam_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Same as above, except arbitrary Gaussians (i.e., each class with its own covariance matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17447916666666666"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(spam_data, np.ravel(spam_labels))\n",
    "test_scikits_model(qda, spam_test_data, spam_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Averaged Percepton w/ Feature Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spam_expanded = np.array([expand_features(x) for x in spam_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_test_expanded = np.array([expand_features(x) for x in spam_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expanded_examples = zip(spam_expanded, spam_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2766927083333333"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_weights = avg_perceptron_train(64, expanded_examples)\n",
    "test_perceptron_model(predict, spam_test_expanded, spam_test_labels, trained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.6 Logistic Regression w/ Feature Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07747395833333333"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(spam_expanded, np.ravel(spam_labels))\n",
    "test_scikits_model(clf, spam_test_expanded, spam_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(data, labels, num_folds):\n",
    "    data_folds = np.array_split(data, num_folds)\n",
    "    label_folds = np.array_split(labels, num_folds)\n",
    "    data_combos = np.array(list(itertools.combinations(data_folds, num_folds - 1)))\n",
    "    label_combos = np.array(list(itertools.combinations(label_folds, num_folds - 1)))\n",
    "    \n",
    "    for fold_num in range(num_folds):\n",
    "        # create the training data\n",
    "        train_data = np.concatenate(data_combos[fold_num])\n",
    "        train_labels = np.concatenate(label_combos[fold_num])\n",
    "        # create the holdout set\n",
    "        test_ix = (num_folds - 1) - fold_num\n",
    "        test_data = data_folds[test_ix]\n",
    "        test_labels = label_folds[test_ix]\n",
    "        training = {'data': train_data, 'labels': train_labels}\n",
    "        test = {'data': test_data, 'labels': test_labels}\n",
    "        \n",
    "        yield (training, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('Averaged Perceptron', AveragedPerceptron),\n",
    "    ('Logistic Regression', LogisticRegression), \n",
    "    ('QDA', QuadraticDiscriminantAnalysis),\n",
    "    ('LDA', LinearDiscriminantAnalysis),\n",
    "    ('Averaged Perceptron Expanded', AveragedPerceptron),\n",
    "    ('Logistic Regression Expanded', LogisticRegression)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_models(models):\n",
    "    for name, Model in models:\n",
    "        errors = []\n",
    "        classifiers = []\n",
    "        num_folds = 10\n",
    "        for training, testing in k_fold_cross_validation(spam_data, spam_labels, num_folds):\n",
    "            if 'Expanded' in name:\n",
    "                training['data'] = np.array([expand_features(x) for x in training['data']])\n",
    "                testing['data'] = np.array([expand_features(x) for x in testing['data']])\n",
    "            clf = Model()\n",
    "            clf.fit(training['data'], training['labels'])\n",
    "            error = test_scikits_model(clf, testing['data'], testing['labels'])\n",
    "            errors.append(error)\n",
    "            classifiers.append(clf)\n",
    "        avg_error = np.sum(errors) / num_folds\n",
    "\n",
    "        yield (name, avg_error, errors, classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_classifer(scored_models):\n",
    "    errors = np.concatenate([s[2] for s in scored_models])\n",
    "    classifiers = np.concatenate([s[3] for s in scored_models])\n",
    "    min_error_ix = np.argmin(errors)\n",
    "    name_ix = min_error_ix//10\n",
    "    clf_name = [m[0] for m in scored_models][name_ix]\n",
    "    return (clf_name, errors[min_error_ix], classifiers[min_error_ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_error_rates(scored_models):\n",
    "    for scored in scored_models:\n",
    "        print \"{} Cross Validation Error Rate: {}\".format(scored[0], scored[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_classfier_test_error_rate(scored_models, test_data, test_labels):\n",
    "    clf_name, clf_error, clf = select_classifer(scored_models)\n",
    "    if 'Expanded' in clf_name:\n",
    "        test_data = np.array([expand_features(x) for x in test_data])\n",
    "    test_error = test_scikits_model(clf, test_data, test_labels)\n",
    "    return test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scored_models = list(score_models(models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_clf_name, best_clf_training_error, best_clf = select_classifer(scored_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_clf_test_error = best_classfier_test_error_rate(\n",
    "    scored, spam_test_data, spam_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged Perceptron Cross Validation Error Rate: 0.136062676971\n",
      "Logistic Regression Cross Validation Error Rate: 0.0815769304464\n",
      "QDA Cross Validation Error Rate: 0.163780843499\n",
      "LDA Cross Validation Error Rate: 0.107669625939\n",
      "Averaged Perceptron Expanded Cross Validation Error Rate: 0.423422963105\n",
      "Logistic Regression Expanded Cross Validation Error Rate: 0.0756956419919\n",
      "Logistic Regression Expanded Training Error Rate: 0.0456\n"
     ]
    }
   ],
   "source": [
    "# 1. Report the cross-validation error rates for all methods\n",
    "cross_validation_error_rates(scored)\n",
    "\n",
    "# 2. the training error rate of the classifier learned by the selected method (and state which method was chosen)\n",
    "print \"%s Training Error Rate: %.4f\" % (best_clf_name, best_clf_training_error)\n",
    "\n",
    "# 3. the test error rate for the learned classifier\n",
    "print \"%s Testing Error Rate %.4f\" % (best_clf_name, best_clf_test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
